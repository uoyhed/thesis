# LSBert: A Simple Framework for Lexical Simplification
# LSBert: 어휘 단순화를 위한 간단한 프레임워크



## Abstract

Lexical simplification (LS) aims to replace complex words in a given sentence with their simpler alternatives of equivalent
meaning, to simplify the sentence. Recently unsupervised lexical simplification approaches only rely on the complex word itself regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious candidates. In this paper, we propose a lexical simplification framework LSBert based on pretrained representation model Bert, that is capable of (1) making use of the wider context when both detecting the words in need of simplification and generating substitue candidates, and (2) taking five high-quality features into account for ranking candidates, including Berts prediction order, Bert-based language model, and the paraphrase database PPDB, in addition to the word frequency and word similarity commonly used in other LS methods. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and obtains obvious improvement compared with these baselines, outperforming the state-of-the-art by 29.8 Accuracy points on three well-known benchmarks.
어휘 단순화(LS)는 주어진 문장의 복잡한 단어를 동등한 의미의 더 간단한 대안으로 대체하여 문장을 단순화하는 것을 목표로 합니다. 최근에는 감독되지 않은 어휘 단순화 접근 방식은 주어진 문장에 관계없이 복잡한 단어 자체에만 의존하여 후보 대체를 생성하므로 필연적으로 많은 수의 가짜 후보가 생성됩니다. 본 논문에서는 사전 훈련된 표현 모델 Bert를 기반으로 하는 어휘 단순화 프레임워크 LSBert를 제안합니다. 이 프레임워크는 (1) 단순화가 필요한 단어를 감지하고 대체 후보를 생성할 때 더 넓은 컨텍스트를 사용하고 (2) 다음을 수행할 수 있습니다. 다른 LS 방법에서 일반적으로 사용되는 단어 빈도 및 단어 유사성 외에도 Berts 예측 순서, Bert 기반 언어 모델 및 의역 데이터베이스 PPDB를 포함하여 순위 후보를 고려한 5가지 고품질 기능. 우리는 우리 시스템이 문법적으로 정확하고 의미적으로 적절한 어휘 단순화를 출력하고 이러한 기준과 비교하여 명백한 개선을 얻었으며 잘 알려진 세 가지 벤치마크에서 29.8 정확도 포인트로 최첨단을 능가함을 보여줍니다.




Lexical Simplification (LS) aims at replacing complex words with simpler alternatives, which can help various groups of people, including children [1], non-native speakers [2], people with cognitive disabilities [3], [4], to understand text better. LS is an effective way of simplifying a text because some work shows that those who are familiar with the vocabulary of a text can often understand its meaning even if the grammatical constructs used are confusing to them. The LS framework is commonly framed as a pipeline of three steps: complex word identification (CWI), substitute generation (SG) of complex words, and filtering and substitute ranking (SR). CWI is often treated as an independent task [5]. Existing LS systems mainly focused on the two steps (SG and SR) [6].
Lexical Simplification(LS)은 복잡한 단어를 더 간단한 대안으로 대체하는 것을 목표로 하며, 이는 어린이[1], 비원어민[2], 인지 장애가 있는 사람들[3], [4]을 포함한 다양한 그룹의 사람들이 이해하는 데 도움이 될 수 있습니다. 더 나은 텍스트. LS는 텍스트를 단순화하는 효과적인 방법입니다. 일부 작업에서는 텍스트의 어휘에 익숙한 사람들이 사용된 문법적 구성이 혼란스럽더라도 종종 그 의미를 이해할 수 있음을 보여주기 때문입니다. LS 프레임워크는 일반적으로 복잡한 단어 식별(CWI), 복잡한 단어의 대체 생성(SG), 필터링 및 대체 순위(SR)의 세 단계로 구성된 파이프라인으로 구성됩니다. CWI는 종종 독립적인 작업으로 취급됩니다[5]. 기존 LS 시스템은 주로 2단계(SG 및 SR)에 중점을 두었습니다[6].




The popular LS systems still predominantly use a set of rules for substituting complex words with their frequent synonyms from carefully handcrafted databases (e.g., WordNet) [7] or automatically induced from comparable corpora [1] or paraphrase database [8]. Recent work utilizes word embedding models to extract substitute candidates for complex words. Given a complex word, they extracted the top 10 words as substitute candidates from the word embedding model whose vectors are closer in terms of cosine similarity with the complex word [2], [5], [9]. Recently, the LS system REC-LS attempts to generate substitute candidates by combining linguistic databases and word embedding models. However, they generated substitute candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that confuse the systems employed in the subsequent steps. For example, if simpler alternatives of the complex word do not exist in substitute candidates, the filtering and substitute ranking step of LS is meaningless.
대중적인 LS 시스템은 복잡한 단어를 주의 깊게 손으로 만든 데이터베이스(예: WordNet)[7] 또는 비교 가능한 말뭉치[1] 또는 의역 데이터베이스[8]에서 빈번한 동의어로 대체하는 규칙 세트를 여전히 주로 사용합니다. 최근 연구는 단어 임베딩 모델을 활용하여 복잡한 단어에 대한 대체 후보를 추출합니다. 복소수 단어가 주어졌을 때, 그들은 복소수 단어와 코사인 유사도 측면에서 벡터가 더 가까운 단어 임베딩 모델에서 상위 10개 단어를 대체 후보로 추출했습니다[2], [5], [9]. 최근 LS 시스템 REC-LS는 언어 데이터베이스와 단어 임베딩 모델을 결합하여 대체 후보를 생성하려고 시도합니다. 그러나 그들은 복잡한 단어의 문맥에 관계없이 복잡한 단어에 대한 대체 후보를 생성했으며, 이는 필연적으로 후속 단계에서 사용되는 시스템을 혼란스럽게 만드는 많은 수의 가짜 후보를 생성할 것입니다. 예를 들어, 복잡한 단어의 더 간단한 대안이 대체 후보에 존재하지 않으면 LS의 필터링 및 대체 순위 단계는 의미가 없습니다.




## 그림 1 ##

Fig. 1. Comparison of substitute candidates of complex words. Given one sentence ”John composed these verses.” and complex words ’composed’ and ’verses’, the top three simplification candidates for each complex word are generated by our method LSBert and the state-of-theart two baselines (Glavaˇs [9] and REC-LS [6]). The simplified sentences by the three LS methods are shown at the bottom.
그림 1. 복합어의 대체 후보 비교. “존이 이 구절들을 지었다”라는 한 문장이 주어졌다. 및 복합 단어 'composed' 및 'verses', 각 복합 단어에 대한 상위 3개 단순화 후보는 우리의 방법인 LSBert와 최신 2개의 기준선(Glavaˇs [9] 및 REC-LS [6])에 의해 생성됩니다. 3가지 LS 방식에 의한 단순화된 문장은 하단에 표시됩니다.



Context plays a central role in fulfilling substitute generation. Here, we give a simple example shown in Figure 1. For complex words ’composed’ and ’verses’ in the sentence ”John composed these verses.”, the top three substitute candidates of the two complex words generated by the state-of-the-art LS systems [6], [9] are only related with the complex words itself regardless of the context. For example, the candidates ”consisting, consists, comprised” is generated by Glavaˇs [9] for the complex word ”composed”, and the candidates ”framed, quieted, planned” is produced by REC-LS [6].
문맥은 대체 생성을 수행하는 데 중심 역할을 합니다. 여기, 우리는 그림 1에 표시된 간단한 예를 제공합니다. "John이 이 구절을 작곡했습니다."라는 문장에서 복잡한 단어 'composed'와 'verses'에 대해, 상태에 의해 생성된 두 개의 복잡한 단어의 상위 3개 대체 후보 - art LS 시스템 [6], [9]은 문맥에 관계없이 복잡한 단어 자체와 관련이 있습니다. 예를 들어, 후보 "consisting, consists, comprised"은 복잡한 단어 "composed"에 대해 Glavaˇs[9]에 의해 생성되고 후보 "framed, quieted, planned"은 REC-LS[6]에 의해 생성됩니다.



In contrast to the existing LS methods that only considered the context in the last step (substitute ranking), we present a novel LS framework LSBert, which takes the context into account in all three steps of LS. As word complexity depends on context, LSBert uses a novel approach to identify complex words using a sequence labeling method [10] based on bi-directional long short-term memory units (BiLSTM). For producing suitable simplifications for the complex word, we exploit recent advances in pretrained unsupervised deep bidirectional representations Bert [11] . More specifically, we mask the complex word w of the original sentence S as a new sentence S0, and concatenate the original sequence S and S0 for feeding into the Bert to obtain the probability distribution of the vocabulary corresponding to the masked word. Then we choose the top probability words as substitute candidates.
마지막 단계(대체 순위)에서 문맥만 고려한 기존 LS 방법과 달리 LS의 세 단계 모두에서 문맥을 고려하는 새로운 LS 프레임워크 LSBert를 제시합니다. 단어 복잡도는 컨텍스트에 따라 달라지므로 LSBert는 양방향 장단기 기억 장치(BiLSTM)에 기반한 시퀀스 레이블링 방법[10]을 사용하여 복잡한 단어를 식별하는 새로운 접근 방식을 사용합니다. 복잡한 단어에 대한 적절한 단순화를 생성하기 위해 사전 훈련된 감독되지 않은 깊은 양방향 표현 Bert [11]의 최근 발전을 활용합니다. 보다 구체적으로, 우리는 원래 문장 S의 복잡한 단어 w를 새로운 문장 S0으로 마스킹하고 원래 시퀀스 S와 S0을 연결하여 Bert에 공급하여 마스크된 단어에 해당하는 어휘의 확률 분포를 얻습니다. 그런 다음 대체 후보로 상위 확률 단어를 선택합니다.



For ranking the substitutions, we adopt five high-quality features including word frequency and word similarity, Berts prediction order, Bert-based language model, and the paraphrase database PPDB, to ensure grammaticality and meaning equivalence to the original sentence in the output. LSBert simplifies one word at a time and is recursively applied to simplify the sentence by taking word complexity in context into account. As shown in Figure 1, the meaning of the original sentence using Glavaˇs is changed, and REC-LS does not make the right simplification. LSBert generates the appropriate substitutes and achieves its aim that replaces complex words with simpler alternatives.
대체 순위를 매기기 위해 단어 빈도 및 단어 유사도, Berts 예측 순서, Bert 기반 언어 모델, 의역 데이터베이스 PPDB를 포함한 5가지 고품질 특성을 채택하여 출력에서 원래 문장과 문법 및 의미 동등성을 보장합니다. LSBert는 한 번에 한 단어를 단순화하고 문맥에서 단어 복잡성을 고려하여 문장을 단순화하기 위해 재귀적으로 적용됩니다. 그림 1과 같이 Glavaˇs를 사용한 원문의 의미가 바뀌었고, REC-LS는 올바른 단순화를 하지 않았다. LSBert는 적절한 대체물을 생성하고 복잡한 단어를 더 간단한 대체물로 대체하는 목표를 달성합니다.



This paper has the following two contributions:
이 문서에는 다음과 같은 두 가지 기여가 있습니다:

(1) LSBert is a novel Bert-based method for LS, which can take full advantages of Bert to generate and rank substitute candidates. To our best knowledge, this is the first attempt to apply pretrained transformer language models for LS. In contrast to existing methods without considering the context in complex word identification and substitute generations, LSBert is easier to hold cohesion and coherence of a sentence, since LSBert takes the context into count for each step of LS
(1) LSBert는 Bert의 장점을 최대한 활용하여 대체 후보를 생성하고 순위를 매길 수 있는 LS의 새로운 Bert 기반 방법입니다. 우리가 아는 한, 이것은 LS에 대해 사전 훈련된 변환기 언어 모델을 적용하려는 첫 번째 시도입니다. 복잡한 단어 식별 및 대체 생성에서 컨텍스트를 고려하지 않는 기존 방법과 달리 LSBert는 LS의 각 단계에서 컨텍스트를 고려하므로 문장의 응집력 및 일관성을 유지하기가 더 쉽습니다.



(2) LSBert is a simple, effective and complete LS method. 1)Simple: many steps used in existing LS systems have been eliminated from our method, e.g., morphological transformation. 2) Effective: it obtains new state-of-the-art results on three benchmarks. 3) Complete: LSBert recursively simplifies all complex words in a sentence without requiring additional steps.
(2) LSBert는 간단하고 효과적이며 완전한 LS 방법입니다. 1) 단순: 기존 LS 시스템에서 사용되는 많은 단계(예: 형태 변환)가 우리 방법에서 제거되었습니다. 2) 효과적: 3가지 벤치마크에서 새로운 최첨단 결과를 얻습니다. 3) 완료: LSBert는 추가 단계 없이 문장의 모든 복잡한 단어를 재귀적으로 단순화합니다.



The rest of this paper is organized as follows. In Section 2, we introduce the related work of text simplification. Section 3 describes the framework LSBert. In Section 4, we describe the experimental setup and evaluate the proposed method LSBert. Finally, we draw our conclusions in Section 5.
이 문서의 나머지 부분은 다음과 같이 구성됩니다. 2장에서는 이와 관련된 텍스트 단순화 작업을 소개한다. 섹션 3에서는 프레임워크 LSBert에 대해 설명합니다. 4장에서는 실험 설정을 설명하고 제안된 방법인 LSBert를 평가합니다. 마지막으로 5장에서 결론을 내린다.



## 2. RELATED WORK ##
## 2. 관련사항 ##

Textual simplification (TS) is the process of simplifying the content of the original text as much as possible, while retaining the meaning and grammaticality so that it can be more easily read and understood by a wider audience. Textual simplification focuses on simplifying the vocabulary and syntax of the text. Early systems of TS often used standard statistical machine translation approaches to learn the simplification of a complex sentence into a simplified sentence [12]. Recently, TS methods adopted encoderdecoder model to simplify the text based on parallel corpora [13]– [15]. All of the above work belong to the supervised TS systems, whose performance strongly relies on the availability of large amounts of parallel sentences. Two public parallel benchmarks WikiSmall [16] and WikiLarge [17] contain a large proportion of: inaccurate simplifications (not aligned or only partially aligned) ; inadequate simplifications (not much simpler) [18], [19]. These problems is mainly because designing a good alignment algorithm for extracting parallel sentences from EW and SEW is very difficult [20]. Therefore, a number of approaches focusing on the generation and assessment of lexical simplification were proposed.
TS(Textual Simplification)는 원문의 내용을 최대한 단순화하고 의미와 문법을 유지하여 더 많은 청중이 더 쉽게 읽고 이해할 수 있도록 하는 과정입니다. 텍스트 단순화는 텍스트의 어휘와 구문을 단순화하는 데 중점을 둡니다. TS의 초기 시스템은 복잡한 문장을 단순화된 문장으로 단순화하기 위해 표준 통계 기계 번역 접근 방식을 자주 사용했습니다[12]. 최근 TS 방법은 병렬 말뭉치 기반의 텍스트 단순화를 위해 인코더 디코더 모델을 채택했습니다[13]-[15]. 위의 모든 작업은 많은 양의 병렬 문장의 가용성에 크게 의존하는 감독된 TS 시스템에 속합니다. 두 개의 공개 병렬 벤치마크 WikiSmall [16] 및 WikiLarge [17]에는 다음이 포함되어 있습니다. 부정확한 단순화(정렬되지 않거나 부분적으로만 정렬) ; 부적절한 단순화(훨씬 단순하지 않음) [18], [19]. 이러한 문제는 주로 EW와 SEW에서 병렬 문장을 추출하기 위한 좋은 정렬 알고리즘을 설계하는 것이 매우 어렵기 때문입니다[20]. 따라서 어휘 단순화의 생성 및 평가에 중점을 둔 여러 접근 방식이 제안되었습니다.



Lexical simplification (LS) only focuses to simplify complex words of one sentence. LS needs to identify complex words and find the best candidate substitution for these complex words [21], [22]. The best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task. The popular lexical simplification approaches were rule-based, in which each rule contains a complex word and its simple synonyms [8], [23], [24]. Rule-based systems usually identified synonyms from Word- Net or other linguistic databases for a predefined set of complex words and selected the ”simplest” from these synonyms based on the frequency of word or length of word [1], [7]. However, there is a major limitation for the rule-based systems that it is impossible to give all possible simplification rules for each word.
어휘 단순화(LS)는 한 문장의 복잡한 단어만 단순화하는 데 중점을 둡니다. LS는 복잡한 단어를 식별하고 이러한 복잡한 단어에 대한 최상의 대체 후보를 찾아야 합니다[21,22]. 가장 좋은 대체는 문장을 문법적으로 보존하고 의미를 최대한 유지하면서 더 단순해야 하는 매우 어려운 작업입니다. 대중적인 어휘 단순화 접근법은 규칙 기반이었고, 각 규칙에는 복잡한 단어와 간단한 동의어가 포함되어 있습니다[8], [23], [24]. 규칙 기반 시스템은 일반적으로 사전 정의된 복잡한 단어 세트에 대해 WordNet 또는 기타 언어 데이터베이스에서 동의어를 식별하고 단어의 빈도 또는 단어 길이에 따라 이러한 동의어에서 "가장 간단한" 것을 선택했습니다[1], [7]. 그러나 각 단어에 대해 가능한 모든 단순화 규칙을 제공하는 것이 불가능하다는 규칙 기반 시스템의 주요 한계가 있습니다.



As complex and simplified parallel corpora are available, LS systems tried to extract rules from parallel corpora [25]–[27]. Yatskar et al. (2010) identified lexical simplifications from the edit history of simple English Wikipedia (SEW). They utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes. Biran et al. (2011) considered every pair of distinct word in the English Wikipedia (EW) and SEW to be a possible simplification pair, and filtered part of them based on morphological variants and WordNet. Horn et al. (2014) also generated the candidate rules from the EW and SEW, and adopted a context-aware binary classifier to decide whether a candidate rule should be adopted or not in a certain context. The main limitation of the type of methods relies heavily on parallel corpora.
복잡하고 단순화된 병렬 말뭉치를 사용할 수 있기 때문에 LS 시스템은 병렬 말뭉치에서 규칙을 추출하려고 했습니다[25]–[27]. Yatskar et al. (2010)은 simple English Wikipedia(SEW)의 편집 기록에서 어휘 단순화를 식별했습니다. 그들은 다른 유형의 콘텐츠 변경과 구별되는 단순화 편집을 인식하기 위해 확률적 방법을 사용했습니다. Biranet al. (2011) English Wikipedia(EW) 및 SEW의 모든 고유한 단어 쌍을 가능한 단순화 쌍으로 간주하고 형태학적 변형 및 WordNet을 기반으로 일부를 필터링했습니다. Horn et al. (2014) 또한 EW 및 SEW에서 후보 규칙을 생성하고 특정 컨텍스트에서 후보 규칙을 채택해야 하는지 여부를 결정하기 위해 context-aware binary classifier(컨텍스트 인식 바이너리 분류기)를 채택했습니다. 메서드 유형의 주요 제한은 병렬 말뭉치에 크게 의존합니다.



To entirely avoid the requirement of lexical resources or parallel corpora, LS systems based on word embeddings were proposed [9]. They extracted the top 10 words as candidate substitutions whose vectors are closer in terms of cosine similarity with the complex word. Instead of a traditional word embedding model, Paetzold and Specia (2016) adopted context-aware word embeddings trained on a large dataset where each word is annotated with the POS tag. Afterward, they further extracted candidates for the complex word by combining word embeddings with WordNet and parallel corpora [5]. REC-LS [6] attempted to generate substitutes from multiple sources, e.g, WordNet, Big Huge Thesaurus 1 and word embeddings.
어휘 자원 또는 병렬 말뭉치의 요구 사항을 완전히 피하기 위해 단어 임베딩을 기반으로 하는 LS 시스템이 제안되었습니다[9]. 그들은 복소수 단어와의 코사인 유사성 측면에서 벡터가 더 가까운 후보 치환으로 상위 10개 단어를 추출했습니다. Paetzold와 Specia(2016)는 전통적인 단어 임베딩 모델 대신 각 단어에 POS 태그가 추가된 대규모 데이터 세트에서 훈련된 컨텍스트 인식 단어 임베딩을 채택했습니다. 이후 단어 임베딩과 WordNet 및 병렬 말뭉치[5]를 결합하여 복잡한 단어에 대한 후보를 추가로 추출했습니다. REC-LS[6]는 WordNet, Big Huge Thesaurus 1 및 단어 임베딩과 같은 여러 소스에서 대체물을 생성하려고 시도했습니다.



After examining existing LS methods ranging from rulesbased to embedding-based, the major challenge is that they generated simplification candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that confuse the systems employed in the subsequent steps.
규칙 기반에서 임베딩 기반에 이르기까지 기존 LS 방법을 검토한 후 주요 문제는 복잡한 단어의 컨텍스트에 관계없이 복잡한 단어에 대한 단순화 후보를 생성한다는 것입니다. 다음 단계에서.



## 그림 2. 어휘 단순화 프레임워크 LSBert의 개요 ##



In this paper, we will first present a LS approach LSBert that requires only a sufficiently large corpus of raw text without any manual efforts. Pre-training language models [11], [28], [29] have attracted wide attention and has shown to be effective for improving many downstream natural language processing tasks. Our method exploits recent advances in Bert to generate suitable simplifications for complex words. Our method generates the candidates of the complex word by considering the whole sentence that is easier to hold cohesion and coherence of a sentence. In this case, many steps used in existing LS methods have been eliminated from our method, e.g., morphological transformation. The previous version LSBert was published in artificial intelligence conference (AAAI) [30], which only focused on substitute generations given the sentence and its complex word using Bert. In this paper, we propose an LS framework including complex word identification, substitute generations, substitute ranking. The framework can simplify one sentence recursively. One recent work for LS based Bert [31] was almost simultaneously proposed with our previous version, which also only focused on substitute generations.
이 논문에서는 먼저 수동 작업 없이 충분히 큰 원시 텍스트 코퍼스만 필요로 하는 LS 접근 방식 LSBert를 제시합니다. 사전 훈련 언어 모델[11], [28], [29]은 많은 관심을 끌었으며 많은 다운스트림 자연어 처리 작업을 개선하는 데 효과적인 것으로 나타났습니다. 우리의 방법은 복잡한 단어에 대한 적절한 단순화를 생성하기 위해 Bert의 최근 발전을 활용합니다. 우리의 방법은 문장의 응집력과 일관성을 유지하기 쉬운 전체 문장을 고려하여 복잡한 단어의 후보를 생성합니다. 이 경우 기존 LS 방법에서 사용된 많은 단계(예: 형태 변환)가 우리 방법에서 제거되었습니다. 이전 버전의 LSBert는 AAAI(Artificial Intelligence Conference)[30]에서 발표되었으며 Bert를 사용하여 문장과 복잡한 단어가 주어진 대체 세대에만 초점을 맞췄습니다. 본 논문에서는 복합어 식별, 대체 생성, 대체 순위를 포함하는 LS 프레임워크를 제안한다. 프레임워크는 한 문장을 재귀적으로 단순화할 수 있습니다. LS 기반 Bert[31]에 대한 최근 작업 중 하나는 대체 세대에만 초점을 맞춘 이전 버전과 거의 동시에 제안되었습니다.



## 3. LEXICAL SIMPLIFICATION FRAMEWORK ##
## 3. 어휘 단순화 프레임워크 ##



In this section, we outline each step of our lexical simplification framework LSBert as presented in Figure 2, which includes the following three steps: complex word identification, substitute generation, filtering and substitute ranking. LSBert simlifies one complex word at a time, and is recursively applied to simplify the sentence. We will give the details of each step below.
이 섹션에서는 복잡한 단어 식별, 대체 생성, 필터링 및 대체 순위의 세 단계를 포함하는 그림 2에 표시된 대로 어휘 단순화 프레임워크 LSBert의 각 단계를 간략하게 설명합니다. LSBert는 한 번에 하나의 복잡한 단어를 단순화하고 재귀적으로 적용하여 문장을 단순화합니다. 아래에서 각 단계에 대한 세부 정보를 제공합니다.



## 3-1. Complex Word Identification (CWI) ##
## 3-1. 복잡한 단어 식별 (CWI) ##



Identifying complex words from one sentence has been studied for years, whose goal is to select the words in a given sentence which should be simplified [32], [33].
한 문장에서 복잡한 단어를 식별하는 것은 단순화되어야 하는 주어진 문장에서 단어를 선택하는 것을 목표로 하는 수년간 연구되어 왔습니다[32], [33].



CWI was framed as a sequence labeling task [10] and an approach SEQ based on bi-directional long short-term memory units (BiLSTM) is trained to predict the binary complexity of words as annotated in the dataset of [34]. In contrast to the other CWI models, the SEQ model has the following two advantages: takes word context into account and helps avoid the necessity of extensive feature engineering, because SEQ only relies on word embeddings as the only input information.
CWI는 시퀀스 라벨링 작업으로 구성되었으며[10] 양방향 장단기 기억 장치(BiLSTM)에 기반한 접근 SEQ는 [34]의 데이터 세트에 주석이 달린 단어의 이진 복잡성을 예측하도록 훈련되었습니다. 다른 CWI 모델과 달리 SEQ 모델은 다음과 같은 두 가지 이점이 있습니다. 단어 컨텍스트를 고려하고 SEQ가 유일한 입력 정보로 단어 임베딩에만 의존하기 때문에 광범위한 기능 엔지니어링의 필요성을 피하는 데 도움이 됩니다.



The SEQ approach labels each word with a lexical complexity score (p) which represents the likelihood of each word belonging to the complex class. Giving a predefined threshold p, if the lexical complexity of one word is greater than the threshold, it will be treated as a complex word. For example, the example ”John composed(0.55) these verses(0.76)” is showed in Figure 2. If the complexity threshold is set to 0.5, the two words ”composed” and ”verses” will be the complex words to be simplified.
SEQ 접근 방식은 각 단어가 복잡한 클래스에 속할 가능성을 나타내는 어휘 복잡도 점수(p)로 각 단어에 레이블을 지정합니다. 미리 정의된 임계값 p를 지정하면 한 단어의 어휘 복잡성이 임계값보다 크면 복잡한 단어로 처리됩니다. 예를 들어, "John composed(0.55) these verses(0.76)"의 예가 그림 2에 나와 있습니다. 복잡도 임계값이 0.5로 설정되면 "composed"와 "verses"라는 두 단어는 단순화할 복잡한 단어가 됩니다.



LSBert starts with the word ”verses” with the highest p value above the predefined threshold to simplify. After completing the simplification process, we will recalculate the complexity of each word in the sentence, excluding words that have been simplified. In addition, we exclude the simplification of entity words by performing named entity identification on the sentence.
LSBert는 단순화하기 위해 미리 정의된 임계값보다 높은 p 값을 가진 "verses"라는 단어로 시작합니다. 단순화 과정을 완료한 후, 우리는 단순화된 단어를 제외하고 문장의 각 단어의 복잡성을 다시 계산합니다. 또한 문장에 대해 개체명 식별을 수행하여 개체어의 단순화를 배제하였다.



