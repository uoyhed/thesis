# LSBert: A Simple Framework for Lexical Simplification
# LSBert: 어휘 단순화를 위한 간단한 프레임워크



## Abstract

Lexical simplification (LS) aims to replace complex words in a given sentence with their simpler alternatives of equivalent
meaning, to simplify the sentence. Recently unsupervised lexical simplification approaches only rely on the complex word itself regardless of the given sentence to generate candidate substitutions, which will inevitably produce a large number of spurious candidates. In this paper, we propose a lexical simplification framework LSBert based on pretrained representation model Bert, that is capable of (1) making use of the wider context when both detecting the words in need of simplification and generating substitue candidates, and (2) taking five high-quality features into account for ranking candidates, including Berts prediction order, Bert-based language model, and the paraphrase database PPDB, in addition to the word frequency and word similarity commonly used in other LS methods. We show that our system outputs lexical simplifications that are grammatically correct and semantically appropriate, and obtains obvious improvement compared with these baselines, outperforming the state-of-the-art by 29.8 Accuracy points on three well-known benchmarks.
어휘 단순화(LS)는 주어진 문장의 복잡한 단어를 동등한 의미의 더 간단한 대안으로 대체하여 문장을 단순화하는 것을 목표로 합니다. 최근에는 감독되지 않은 어휘 단순화 접근 방식은 주어진 문장에 관계없이 복잡한 단어 자체에만 의존하여 후보 대체를 생성하므로 필연적으로 많은 수의 가짜 후보가 생성됩니다. 본 논문에서는 사전 훈련된 표현 모델 Bert를 기반으로 하는 어휘 단순화 프레임워크 LSBert를 제안합니다. 이 프레임워크는 (1) 단순화가 필요한 단어를 감지하고 대체 후보를 생성할 때 더 넓은 컨텍스트를 사용하고 (2) 다음을 수행할 수 있습니다. 다른 LS 방법에서 일반적으로 사용되는 단어 빈도 및 단어 유사성 외에도 Berts 예측 순서, Bert 기반 언어 모델 및 의역 데이터베이스 PPDB를 포함하여 순위 후보를 고려한 5가지 고품질 기능. 우리는 우리 시스템이 문법적으로 정확하고 의미적으로 적절한 어휘 단순화를 출력하고 이러한 기준과 비교하여 명백한 개선을 얻었으며 잘 알려진 세 가지 벤치마크에서 29.8 정확도 포인트로 최첨단을 능가함을 보여줍니다.




Lexical Simplification (LS) aims at replacing complex words with simpler alternatives, which can help various groups of people, including children [1], non-native speakers [2], people with cognitive disabilities [3], [4], to understand text better. LS is an effective way of simplifying a text because some work shows that those who are familiar with the vocabulary of a text can often understand its meaning even if the grammatical constructs used are confusing to them. The LS framework is commonly framed as a pipeline of three steps: complex word identification (CWI), substitute generation (SG) of complex words, and filtering and substitute ranking (SR). CWI is often treated as an independent task [5]. Existing LS systems mainly focused on the two steps (SG and SR) [6].
Lexical Simplification(LS)은 복잡한 단어를 더 간단한 대안으로 대체하는 것을 목표로 하며, 이는 어린이[1], 비원어민[2], 인지 장애가 있는 사람들[3], [4]을 포함한 다양한 그룹의 사람들이 이해하는 데 도움이 될 수 있습니다. 더 나은 텍스트. LS는 텍스트를 단순화하는 효과적인 방법입니다. 일부 작업에서는 텍스트의 어휘에 익숙한 사람들이 사용된 문법적 구성이 혼란스럽더라도 종종 그 의미를 이해할 수 있음을 보여주기 때문입니다. LS 프레임워크는 일반적으로 복잡한 단어 식별(CWI), 복잡한 단어의 대체 생성(SG), 필터링 및 대체 순위(SR)의 세 단계로 구성된 파이프라인으로 구성됩니다. CWI는 종종 독립적인 작업으로 취급됩니다[5]. 기존 LS 시스템은 주로 2단계(SG 및 SR)에 중점을 두었습니다[6].




The popular LS systems still predominantly use a set of rules for substituting complex words with their frequent synonyms from carefully handcrafted databases (e.g., WordNet) [7] or automatically induced from comparable corpora [1] or paraphrase database [8]. Recent work utilizes word embedding models to extract substitute candidates for complex words. Given a complex word, they extracted the top 10 words as substitute candidates from the word embedding model whose vectors are closer in terms of cosine similarity with the complex word [2], [5], [9]. Recently, the LS system REC-LS attempts to generate substitute candidates by combining linguistic databases and word embedding models. However, they generated substitute candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that confuse the systems employed in the subsequent steps. For example, if simpler alternatives of the complex word do not exist in substitute candidates, the filtering and substitute ranking step of LS is meaningless.
대중적인 LS 시스템은 복잡한 단어를 주의 깊게 손으로 만든 데이터베이스(예: WordNet)[7] 또는 비교 가능한 말뭉치[1] 또는 의역 데이터베이스[8]에서 빈번한 동의어로 대체하는 규칙 세트를 여전히 주로 사용합니다. 최근 연구는 단어 임베딩 모델을 활용하여 복잡한 단어에 대한 대체 후보를 추출합니다. 복소수 단어가 주어졌을 때, 그들은 복소수 단어와 코사인 유사도 측면에서 벡터가 더 가까운 단어 임베딩 모델에서 상위 10개 단어를 대체 후보로 추출했습니다[2], [5], [9]. 최근 LS 시스템 REC-LS는 언어 데이터베이스와 단어 임베딩 모델을 결합하여 대체 후보를 생성하려고 시도합니다. 그러나 그들은 복잡한 단어의 문맥에 관계없이 복잡한 단어에 대한 대체 후보를 생성했으며, 이는 필연적으로 후속 단계에서 사용되는 시스템을 혼란스럽게 만드는 많은 수의 가짜 후보를 생성할 것입니다. 예를 들어, 복잡한 단어의 더 간단한 대안이 대체 후보에 존재하지 않으면 LS의 필터링 및 대체 순위 단계는 의미가 없습니다.




## 그림 1 ##

Fig. 1. Comparison of substitute candidates of complex words. Given one sentence ”John composed these verses.” and complex words ’composed’ and ’verses’, the top three simplification candidates for each complex word are generated by our method LSBert and the state-of-theart two baselines (Glavaˇs [9] and REC-LS [6]). The simplified sentences by the three LS methods are shown at the bottom.
그림 1. 복합어의 대체 후보 비교. “존이 이 구절들을 지었다”라는 한 문장이 주어졌다. 및 복합 단어 'composed' 및 'verses', 각 복합 단어에 대한 상위 3개 단순화 후보는 우리의 방법인 LSBert와 최신 2개의 기준선(Glavaˇs [9] 및 REC-LS [6])에 의해 생성됩니다. 3가지 LS 방식에 의한 단순화된 문장은 하단에 표시됩니다.



Context plays a central role in fulfilling substitute generation. Here, we give a simple example shown in Figure 1. For complex words ’composed’ and ’verses’ in the sentence ”John composed these verses.”, the top three substitute candidates of the two complex words generated by the state-of-the-art LS systems [6], [9] are only related with the complex words itself regardless of the context. For example, the candidates ”consisting, consists, comprised” is generated by Glavaˇs [9] for the complex word ”composed”, and the candidates ”framed, quieted, planned” is produced by REC-LS [6].
문맥은 대체 생성을 수행하는 데 중심 역할을 합니다. 여기, 우리는 그림 1에 표시된 간단한 예를 제공합니다. "John이 이 구절을 작곡했습니다."라는 문장에서 복잡한 단어 'composed'와 'verses'에 대해, 상태에 의해 생성된 두 개의 복잡한 단어의 상위 3개 대체 후보 - art LS 시스템 [6], [9]은 문맥에 관계없이 복잡한 단어 자체와 관련이 있습니다. 예를 들어, 후보 "consisting, consists, comprised"은 복잡한 단어 "composed"에 대해 Glavaˇs[9]에 의해 생성되고 후보 "framed, quieted, planned"은 REC-LS[6]에 의해 생성됩니다.



In contrast to the existing LS methods that only considered the context in the last step (substitute ranking), we present a novel LS framework LSBert, which takes the context into account in all three steps of LS. As word complexity depends on context, LSBert uses a novel approach to identify complex words using a sequence labeling method [10] based on bi-directional long short-term memory units (BiLSTM). For producing suitable simplifications for the complex word, we exploit recent advances in pretrained unsupervised deep bidirectional representations Bert [11] . More specifically, we mask the complex word w of the original sentence S as a new sentence S0, and concatenate the original sequence S and S0 for feeding into the Bert to obtain the probability distribution of the vocabulary corresponding to the masked word. Then we choose the top probability words as substitute candidates.
마지막 단계(대체 순위)에서 문맥만 고려한 기존 LS 방법과 달리 LS의 세 단계 모두에서 문맥을 고려하는 새로운 LS 프레임워크 LSBert를 제시합니다. 단어 복잡도는 컨텍스트에 따라 달라지므로 LSBert는 양방향 장단기 기억 장치(BiLSTM)에 기반한 시퀀스 레이블링 방법[10]을 사용하여 복잡한 단어를 식별하는 새로운 접근 방식을 사용합니다. 복잡한 단어에 대한 적절한 단순화를 생성하기 위해 사전 훈련된 감독되지 않은 깊은 양방향 표현 Bert [11]의 최근 발전을 활용합니다. 보다 구체적으로, 우리는 원래 문장 S의 복잡한 단어 w를 새로운 문장 S0으로 마스킹하고 원래 시퀀스 S와 S0을 연결하여 Bert에 공급하여 마스크된 단어에 해당하는 어휘의 확률 분포를 얻습니다. 그런 다음 대체 후보로 상위 확률 단어를 선택합니다.



For ranking the substitutions, we adopt five high-quality features including word frequency and word similarity, Berts prediction order, Bert-based language model, and the paraphrase database PPDB, to ensure grammaticality and meaning equivalence to the original sentence in the output. LSBert simplifies one word at a time and is recursively applied to simplify the sentence by taking word complexity in context into account. As shown in Figure 1, the meaning of the original sentence using Glavaˇs is changed, and REC-LS does not make the right simplification. LSBert generates the appropriate substitutes and achieves its aim that replaces complex words with simpler alternatives.
대체 순위를 매기기 위해 단어 빈도 및 단어 유사도, Berts 예측 순서, Bert 기반 언어 모델, 의역 데이터베이스 PPDB를 포함한 5가지 고품질 특성을 채택하여 출력에서 원래 문장과 문법 및 의미 동등성을 보장합니다. LSBert는 한 번에 한 단어를 단순화하고 문맥에서 단어 복잡성을 고려하여 문장을 단순화하기 위해 재귀적으로 적용됩니다. 그림 1과 같이 Glavaˇs를 사용한 원문의 의미가 바뀌었고, REC-LS는 올바른 단순화를 하지 않았다. LSBert는 적절한 대체물을 생성하고 복잡한 단어를 더 간단한 대체물로 대체하는 목표를 달성합니다.



This paper has the following two contributions:
이 문서에는 다음과 같은 두 가지 기여가 있습니다:

(1) LSBert is a novel Bert-based method for LS, which can take full advantages of Bert to generate and rank substitute candidates. To our best knowledge, this is the first attempt to apply pretrained transformer language models for LS. In contrast to existing methods without considering the context in complex word identification and substitute generations, LSBert is easier to hold cohesion and coherence of a sentence, since LSBert takes the context into count for each step of LS
(1) LSBert는 Bert의 장점을 최대한 활용하여 대체 후보를 생성하고 순위를 매길 수 있는 LS의 새로운 Bert 기반 방법입니다. 우리가 아는 한, 이것은 LS에 대해 사전 훈련된 변환기 언어 모델을 적용하려는 첫 번째 시도입니다. 복잡한 단어 식별 및 대체 생성에서 컨텍스트를 고려하지 않는 기존 방법과 달리 LSBert는 LS의 각 단계에서 컨텍스트를 고려하므로 문장의 응집력 및 일관성을 유지하기가 더 쉽습니다.



(2) LSBert is a simple, effective and complete LS method. 1)Simple: many steps used in existing LS systems have been eliminated from our method, e.g., morphological transformation. 2) Effective: it obtains new state-of-the-art results on three benchmarks. 3) Complete: LSBert recursively simplifies all complex words in a sentence without requiring additional steps.
(2) LSBert는 간단하고 효과적이며 완전한 LS 방법입니다. 1) 단순: 기존 LS 시스템에서 사용되는 많은 단계(예: 형태 변환)가 우리 방법에서 제거되었습니다. 2) 효과적: 3가지 벤치마크에서 새로운 최첨단 결과를 얻습니다. 3) 완료: LSBert는 추가 단계 없이 문장의 모든 복잡한 단어를 재귀적으로 단순화합니다.



The rest of this paper is organized as follows. In Section 2, we introduce the related work of text simplification. Section 3 describes the framework LSBert. In Section 4, we describe the experimental setup and evaluate the proposed method LSBert. Finally, we draw our conclusions in Section 5.
이 문서의 나머지 부분은 다음과 같이 구성됩니다. 2장에서는 이와 관련된 텍스트 단순화 작업을 소개한다. 섹션 3에서는 프레임워크 LSBert에 대해 설명합니다. 4장에서는 실험 설정을 설명하고 제안된 방법인 LSBert를 평가합니다. 마지막으로 5장에서 결론을 내린다.



## 2. RELATED WORK ##
## 2. 관련사항 ##

Textual simplification (TS) is the process of simplifying the content of the original text as much as possible, while retaining the meaning and grammaticality so that it can be more easily read and understood by a wider audience. Textual simplification focuses on simplifying the vocabulary and syntax of the text. Early systems of TS often used standard statistical machine translation approaches to learn the simplification of a complex sentence into a simplified sentence [12]. Recently, TS methods adopted encoderdecoder model to simplify the text based on parallel corpora [13]– [15]. All of the above work belong to the supervised TS systems, whose performance strongly relies on the availability of large amounts of parallel sentences. Two public parallel benchmarks WikiSmall [16] and WikiLarge [17] contain a large proportion of: inaccurate simplifications (not aligned or only partially aligned) ; inadequate simplifications (not much simpler) [18], [19]. These problems is mainly because designing a good alignment algorithm for extracting parallel sentences from EW and SEW is very difficult [20]. Therefore, a number of approaches focusing on the generation and assessment of lexical simplification were proposed.
TS(Textual Simplification)는 원문의 내용을 최대한 단순화하고 의미와 문법을 유지하여 더 많은 청중이 더 쉽게 읽고 이해할 수 있도록 하는 과정입니다. 텍스트 단순화는 텍스트의 어휘와 구문을 단순화하는 데 중점을 둡니다. TS의 초기 시스템은 복잡한 문장을 단순화된 문장으로 단순화하기 위해 표준 통계 기계 번역 접근 방식을 자주 사용했습니다[12]. 최근 TS 방법은 병렬 말뭉치 기반의 텍스트 단순화를 위해 인코더 디코더 모델을 채택했습니다[13]-[15]. 위의 모든 작업은 많은 양의 병렬 문장의 가용성에 크게 의존하는 감독된 TS 시스템에 속합니다. 두 개의 공개 병렬 벤치마크 WikiSmall [16] 및 WikiLarge [17]에는 다음이 포함되어 있습니다. 부정확한 단순화(정렬되지 않거나 부분적으로만 정렬) ; 부적절한 단순화(훨씬 단순하지 않음) [18], [19]. 이러한 문제는 주로 EW와 SEW에서 병렬 문장을 추출하기 위한 좋은 정렬 알고리즘을 설계하는 것이 매우 어렵기 때문입니다[20]. 따라서 어휘 단순화의 생성 및 평가에 중점을 둔 여러 접근 방식이 제안되었습니다.



Lexical simplification (LS) only focuses to simplify complex words of one sentence. LS needs to identify complex words and find the best candidate substitution for these complex words [21], [22]. The best substitution needs to be more simplistic while preserving the sentence grammatically and keeping its meaning as much as possible, which is a very challenging task. The popular lexical simplification approaches were rule-based, in which each rule contains a complex word and its simple synonyms [8], [23], [24]. Rule-based systems usually identified synonyms from Word- Net or other linguistic databases for a predefined set of complex words and selected the ”simplest” from these synonyms based on the frequency of word or length of word [1], [7]. However, there is a major limitation for the rule-based systems that it is impossible to give all possible simplification rules for each word.
어휘 단순화(LS)는 한 문장의 복잡한 단어만 단순화하는 데 중점을 둡니다. LS는 복잡한 단어를 식별하고 이러한 복잡한 단어에 대한 최상의 대체 후보를 찾아야 합니다[21,22]. 가장 좋은 대체는 문장을 문법적으로 보존하고 의미를 최대한 유지하면서 더 단순해야 하는 매우 어려운 작업입니다. 대중적인 어휘 단순화 접근법은 규칙 기반이었고, 각 규칙에는 복잡한 단어와 간단한 동의어가 포함되어 있습니다[8], [23], [24]. 규칙 기반 시스템은 일반적으로 사전 정의된 복잡한 단어 세트에 대해 WordNet 또는 기타 언어 데이터베이스에서 동의어를 식별하고 단어의 빈도 또는 단어 길이에 따라 이러한 동의어에서 "가장 간단한" 것을 선택했습니다[1], [7]. 그러나 각 단어에 대해 가능한 모든 단순화 규칙을 제공하는 것이 불가능하다는 규칙 기반 시스템의 주요 한계가 있습니다.



As complex and simplified parallel corpora are available, LS systems tried to extract rules from parallel corpora [25]–[27]. Yatskar et al. (2010) identified lexical simplifications from the edit history of simple English Wikipedia (SEW). They utilized a probabilistic method to recognize simplification edits distinguishing from other types of content changes. Biran et al. (2011) considered every pair of distinct word in the English Wikipedia (EW) and SEW to be a possible simplification pair, and filtered part of them based on morphological variants and WordNet. Horn et al. (2014) also generated the candidate rules from the EW and SEW, and adopted a context-aware binary classifier to decide whether a candidate rule should be adopted or not in a certain context. The main limitation of the type of methods relies heavily on parallel corpora.
복잡하고 단순화된 병렬 말뭉치를 사용할 수 있기 때문에 LS 시스템은 병렬 말뭉치에서 규칙을 추출하려고 했습니다[25]–[27]. Yatskar et al. (2010)은 simple English Wikipedia(SEW)의 편집 기록에서 어휘 단순화를 식별했습니다. 그들은 다른 유형의 콘텐츠 변경과 구별되는 단순화 편집을 인식하기 위해 확률적 방법을 사용했습니다. Biranet al. (2011) English Wikipedia(EW) 및 SEW의 모든 고유한 단어 쌍을 가능한 단순화 쌍으로 간주하고 형태학적 변형 및 WordNet을 기반으로 일부를 필터링했습니다. Horn et al. (2014) 또한 EW 및 SEW에서 후보 규칙을 생성하고 특정 컨텍스트에서 후보 규칙을 채택해야 하는지 여부를 결정하기 위해 context-aware binary classifier(컨텍스트 인식 바이너리 분류기)를 채택했습니다. 메서드 유형의 주요 제한은 병렬 말뭉치에 크게 의존합니다.



To entirely avoid the requirement of lexical resources or parallel corpora, LS systems based on word embeddings were proposed [9]. They extracted the top 10 words as candidate substitutions whose vectors are closer in terms of cosine similarity with the complex word. Instead of a traditional word embedding model, Paetzold and Specia (2016) adopted context-aware word embeddings trained on a large dataset where each word is annotated with the POS tag. Afterward, they further extracted candidates for the complex word by combining word embeddings with WordNet and parallel corpora [5]. REC-LS [6] attempted to generate substitutes from multiple sources, e.g, WordNet, Big Huge Thesaurus 1 and word embeddings.
어휘 자원 또는 병렬 말뭉치의 요구 사항을 완전히 피하기 위해 단어 임베딩을 기반으로 하는 LS 시스템이 제안되었습니다[9]. 그들은 복소수 단어와의 코사인 유사성 측면에서 벡터가 더 가까운 후보 치환으로 상위 10개 단어를 추출했습니다. Paetzold와 Specia(2016)는 전통적인 단어 임베딩 모델 대신 각 단어에 POS 태그가 추가된 대규모 데이터 세트에서 훈련된 컨텍스트 인식 단어 임베딩을 채택했습니다. 이후 단어 임베딩과 WordNet 및 병렬 말뭉치[5]를 결합하여 복잡한 단어에 대한 후보를 추가로 추출했습니다. REC-LS[6]는 WordNet, Big Huge Thesaurus 1 및 단어 임베딩과 같은 여러 소스에서 대체물을 생성하려고 시도했습니다.



After examining existing LS methods ranging from rulesbased to embedding-based, the major challenge is that they generated simplification candidates for the complex word regardless of the context of the complex word, which will inevitably produce a large number of spurious candidates that confuse the systems employed in the subsequent steps.
규칙 기반에서 임베딩 기반에 이르기까지 기존 LS 방법을 검토한 후 주요 문제는 복잡한 단어의 컨텍스트에 관계없이 복잡한 단어에 대한 단순화 후보를 생성한다는 것입니다. 다음 단계에서.



## 그림 2. 어휘 단순화 프레임워크 LSBert의 개요 ##



In this paper, we will first present a LS approach LSBert that requires only a sufficiently large corpus of raw text without any manual efforts. Pre-training language models [11], [28], [29] have attracted wide attention and has shown to be effective for improving many downstream natural language processing tasks. Our method exploits recent advances in Bert to generate suitable simplifications for complex words. Our method generates the candidates of the complex word by considering the whole sentence that is easier to hold cohesion and coherence of a sentence. In this case, many steps used in existing LS methods have been eliminated from our method, e.g., morphological transformation. The previous version LSBert was published in artificial intelligence conference (AAAI) [30], which only focused on substitute generations given the sentence and its complex word using Bert. In this paper, we propose an LS framework including complex word identification, substitute generations, substitute ranking. The framework can simplify one sentence recursively. One recent work for LS based Bert [31] was almost simultaneously proposed with our previous version, which also only focused on substitute generations.
이 논문에서는 먼저 수동 작업 없이 충분히 큰 원시 텍스트 코퍼스만 필요로 하는 LS 접근 방식 LSBert를 제시합니다. 사전 훈련 언어 모델[11], [28], [29]은 많은 관심을 끌었으며 많은 다운스트림 자연어 처리 작업을 개선하는 데 효과적인 것으로 나타났습니다. 우리의 방법은 복잡한 단어에 대한 적절한 단순화를 생성하기 위해 Bert의 최근 발전을 활용합니다. 우리의 방법은 문장의 응집력과 일관성을 유지하기 쉬운 전체 문장을 고려하여 복잡한 단어의 후보를 생성합니다. 이 경우 기존 LS 방법에서 사용된 많은 단계(예: 형태 변환)가 우리 방법에서 제거되었습니다. 이전 버전의 LSBert는 AAAI(Artificial Intelligence Conference)[30]에서 발표되었으며 Bert를 사용하여 문장과 복잡한 단어가 주어진 대체 세대에만 초점을 맞췄습니다. 본 논문에서는 복합어 식별, 대체 생성, 대체 순위를 포함하는 LS 프레임워크를 제안한다. 프레임워크는 한 문장을 재귀적으로 단순화할 수 있습니다. LS 기반 Bert[31]에 대한 최근 작업 중 하나는 대체 세대에만 초점을 맞춘 이전 버전과 거의 동시에 제안되었습니다.



## 3. LEXICAL SIMPLIFICATION FRAMEWORK ##
## 3. 어휘 단순화 프레임워크 ##



In this section, we outline each step of our lexical simplification framework LSBert as presented in Figure 2, which includes the following three steps: complex word identification, substitute generation, filtering and substitute ranking. LSBert simlifies one complex word at a time, and is recursively applied to simplify the sentence. We will give the details of each step below.
이 섹션에서는 복잡한 단어 식별, 대체 생성, 필터링 및 대체 순위의 세 단계를 포함하는 그림 2에 표시된 대로 어휘 단순화 프레임워크 LSBert의 각 단계를 간략하게 설명합니다. LSBert는 한 번에 하나의 복잡한 단어를 단순화하고 재귀적으로 적용하여 문장을 단순화합니다. 아래에서 각 단계에 대한 세부 정보를 제공합니다.



## 3-1. Complex Word Identification (CWI) ##
## 3-1. 복잡한 단어 식별 (CWI) ##



Identifying complex words from one sentence has been studied for years, whose goal is to select the words in a given sentence which should be simplified [32], [33].
한 문장에서 복잡한 단어를 식별하는 것은 단순화되어야 하는 주어진 문장에서 단어를 선택하는 것을 목표로 하는 수년간 연구되어 왔습니다[32], [33].



CWI was framed as a sequence labeling task [10] and an approach SEQ based on bi-directional long short-term memory units (BiLSTM) is trained to predict the binary complexity of words as annotated in the dataset of [34]. In contrast to the other CWI models, the SEQ model has the following two advantages: takes word context into account and helps avoid the necessity of extensive feature engineering, because SEQ only relies on word embeddings as the only input information.
CWI는 시퀀스 라벨링 작업으로 구성되었으며[10] 양방향 장단기 기억 장치(BiLSTM)에 기반한 접근 SEQ는 [34]의 데이터 세트에 주석이 달린 단어의 이진 복잡성을 예측하도록 훈련되었습니다. 다른 CWI 모델과 달리 SEQ 모델은 다음과 같은 두 가지 이점이 있습니다. 단어 컨텍스트를 고려하고 SEQ가 유일한 입력 정보로 단어 임베딩에만 의존하기 때문에 광범위한 기능 엔지니어링의 필요성을 피하는 데 도움이 됩니다.



The SEQ approach labels each word with a lexical complexity score (p) which represents the likelihood of each word belonging to the complex class. Giving a predefined threshold p, if the lexical complexity of one word is greater than the threshold, it will be treated as a complex word. For example, the example ”John composed(0.55) these verses(0.76)” is showed in Figure 2. If the complexity threshold is set to 0.5, the two words ”composed” and ”verses” will be the complex words to be simplified.
SEQ 접근 방식은 각 단어가 복잡한 클래스에 속할 가능성을 나타내는 어휘 복잡도 점수(p)로 각 단어에 레이블을 지정합니다. 미리 정의된 임계값 p를 지정하면 한 단어의 어휘 복잡성이 임계값보다 크면 복잡한 단어로 처리됩니다. 예를 들어, "John composed(0.55) these verses(0.76)"의 예가 그림 2에 나와 있습니다. 복잡도 임계값이 0.5로 설정되면 "composed"와 "verses"라는 두 단어는 단순화할 복잡한 단어가 됩니다.



LSBert starts with the word ”verses” with the highest p value above the predefined threshold to simplify. After completing the simplification process, we will recalculate the complexity of each word in the sentence, excluding words that have been simplified. In addition, we exclude the simplification of entity words by performing named entity identification on the sentence.
LSBert는 단순화하기 위해 미리 정의된 임계값보다 높은 p 값을 가진 "verses"라는 단어로 시작합니다. 단순화 과정을 완료한 후, 우리는 단순화된 단어를 제외하고 문장의 각 단어의 복잡성을 다시 계산합니다. 또한 문장에 대해 개체명 식별을 수행하여 개체어의 단순화를 배제하였다.



## 3-2. Substitute Generation (SG)
## 3-2. 대체 생성 (SG)



Given a sentence S and the complex word w, the aim of substitution generation (SG) is to produce the substitute candidates for the complex word w. LSBert produces the substitute candidates for the complex word using pretrained language model Bert. we briefly summarize the Bert model, and then describe how we extend it to do lexical simplification.
문장 S와 복합어 w가 주어지면 대체 생성(SG)의 목적은 복합어 w에 대한 대체 후보를 생성하는 것입니다. LSBert는 사전 훈련된 언어 모델 Bert를 사용하여 복잡한 단어에 대한 대체 후보를 생성합니다. Bert 모델을 간략하게 요약한 다음 이를 확장하여 어휘 단순화를 수행하는 방법을 설명합니다.



Bert [11] is a self-supervised method for pretrained a deep transformer encoder, which is optimized by two training objectives: masked language modeling (MLM) and next sentence prediction (NSP). Unlike a traditional language modeling objective of predicting the next word in a sequence given the history, MLM predicts missing tokens in a sequence given its left and right context. Bert accomplishes NSP task by prepending every sentence with a special classification token, [CLS], and by combining sentences with a special separator token, [SEP]. The final hidden state corresponding to the [CLS] token is used as the total sequence representation from which we predict a label for classification tasks, or which may otherwise be overlooked.
Bert [11]는 MLM(Masked Language Modeling)과 NSP(다음 문장 예측)라는 두 가지 훈련 목표에 의해 최적화된 심층 transformer encoder를 사전 훈련하기 위한 자체 감독 방법입니다. 주어진 기록이 있는 시퀀스에서 다음 단어를 예측하는 기존의 언어 모델링 목표와 달리 MLM은 왼쪽 및 오른쪽 컨텍스트가 주어지면 시퀀스에서 누락된 토큰을 예측합니다. Bert는 모든 문장 앞에 특수 분류 토큰 [CLS]을 추가하고 문장을 특수 구분 토큰 [SEP]으로 결합하여 NSP 작업을 수행합니다. [CLS] 토큰에 해당하는 최종 숨겨진 상태는 분류 작업에 대한 레이블을 예측하거나 간과할 수 있는 전체 시퀀스 표현으로 사용됩니다.



Due to the fundamental nature of MLM, we mask the complex word w of the sentence S and get the probability distribution of the vocabulary p(ㆍ|S\{w}) corresponding to the masked word w. Therefore, we can try to use MLM for substitute generation.
MLM의 근본적인 특성으로 인해 문장 S의 복잡한 단어 w를 마스킹하고 마스킹된 단어 w에 해당하는 어휘 p(ㆍ|S\{w})의 확률 분포를 얻습니다. 따라서 우리는 대체 생성을 위해 MLM을 사용하려고 할 수 있습니다.



For the complex word w in a sentence S, we mask the word w of S using special symbol ”[MASK]” as a new sequence S0. If we directly feed S0 into MLM, the probability of the vocabulary p(·|S`\{ti}) corresponding to the complex word w only considers the context regardless of the influence of the complex word w. Considering that Bert is adept at dealing with sentence pairs due to the NSP task adopted by Bert. We concatenate the original sequence S and S0 as a sentence pair, and feed the sentence pair (S, S0) into the Bert to obtain the probability distribution of the vocabulary p(·|S,S`\{w}) corresponding to the mask word. Thus, the higher probability words in p(·|S,S`\{w}) corresponding to the mask word not only consider the complex word itself, but also fit the context of the complex word.
문장 S의 복잡한 단어 w에 대해 특수 기호 "[MASK]"를 새 시퀀스 S0으로 사용하여 S의 단어 w를 마스킹합니다. S0를 MLM에 직접 입력하면 복합어 w에 해당하는 어휘 p(·|S`\{ti})의 확률은 복합어 w의 영향에 관계없이 문맥만을 고려합니다. Bert가 채택한 NSP 작업으로 인해 Bert가 문장 쌍을 처리하는 데 능숙하다는 점을 고려하면. 원래 시퀀스 S와 S0를 문장 쌍으로 연결하고 문장 쌍 (S, S0)을 Bert에 공급하여 해당하는 어휘 p(·|S,S`\{w})의 확률 분포를 얻습니다. 마스크 단어. 따라서 마스크 단어에 해당하는 p(·|S,S`\{w})에서 확률이 높은 단어는 복합어 자체를 고려할 뿐만 아니라 복합어의 문맥에도 적합합니다.



Finally, we select the top 10 words from p(·|S,S`\{w}) as substitution candidates, excluding the morphological derivations of w. In addition, considering that the contextual information of the complex word is used twice, we randomly mask a certain percentage of words in S excluding w for appropriately reducing the impact of contextual information.
마지막으로 p(·|S,S`\{w})에서 상위 10개 단어를 w의 형태학적 파생물을 제외하고 대체 후보로 선택합니다. 또한 복합어의 문맥정보가 2회 사용되는 것을 고려하여 문맥정보의 영향을 적절하게 줄이기 위해 w를 제외한 S의 특정 비율의 단어를 무작위로 마스킹한다.



## 그림 3 ##

Fig. 3. Substitution generation of LSBert for the target complex word prediction, or cloze task. The input text is ”the cat perched on the mat” with complex word ”perched”. [MASK], [CLS] and [SEP] are thress special symbols in Bert, where [MASK] is used to mask the word, [CLS] is added in front of each input instance and [SEP] is a special separator token.
그림 3. target 복합어 예측 또는 클로즈 작업(중간 중간 빈칸이 나 있는 글을 읽고 빈칸에 알맞은 말을 써 넣는 작업)에 대한 LSBert의 대체 생성. 입력 텍스트는 복합어 "perched"가 포함된 "the cat perched on the mat"입니다. [MASK], [CLS] 및 [SEP]는 Bert의 특수 기호이며, 여기서 [MASK]는 단어를 마스킹하는 데 사용되며, [CLS]는 각 입력 인스턴스 앞에 추가되며 [SEP]는 특수 구분 기호 토큰입니다.



See Figure 3 for an illustration. Suppose that there is a sentence”the cat perched on the mat” and the complex word ”perched”, we get the top three substitute candidates ”sat, seated, hopped”. We can see that the three candidates not only have a strong correlation with the complex word, but also hold the cohesion and coherence properties of the sentence. If we adopt the existing state-of-the-art methods [9] and [6], the top three substitution words are ”atop, overlooking, precariously” and ”put, lighted, lay”, respectively. Very obviously, our method generates better substitute candidates for the complex word.
그림 3을 참조하십시오. "the cat perched on the mat"라는 문장과 복잡한 단어 "perched"가 있다고 가정하면 상위 3명의 대체 후보인 "sat, seated, hopped"를 얻습니다. 세 후보가 복잡한 단어와 강한 상관관계를 가질 뿐만 아니라 문장의 응집력과 일관성 속성을 가지고 있음을 알 수 있습니다. 기존의 최첨단 방식[9]과 [6]을 채택하면 상위 3개 대체 단어는 각각 'atop, overlooking, precariously'와 'put, lighted, lay'입니다. 매우 분명히, 우리의 방법은 복잡한 단어에 대한 더 나은 대체 후보를 생성합니다.



## 3-3. Filtering and Substitute Ranking (SR)
## 3-3. 필터링 그리고 대체 순위 (SR)

Giving substitute candidates C = {c1, c2, …, Cn}, the substitution ranking of the lexical simplification framework is to decide which one of the candidate substitutions that fits the context of complex word is the simplest [22], where n is the number of substitute candidates. First, threshold-based filtering is performed by LSBert, which is used to remove some complex substitutes. Substitutes are removed from consideration if their Zipf values below 3 using Frequency features. Then, LSBert computes various rankings according to their scores for each of the features. After obtaining all rankings for each feature, LSBert scores each candidate by averaging all its rankings. Finally, we choose the candidate with the highest ranking as the best substitute.
대체 후보 제공 C = {c1, c2, …, Cn},
어휘 단순화 프레임워크의 치환 순위는 복잡한 단어의 문맥에 맞는 후보 대체 중 어느 것이 가장 간단한지를 결정하는 것입니다[22]. 여기서 n은 대체 후보의 수입니다. 첫째, 임계값 기반 필터링은 LSBert에 의해 수행되며, 이는 일부 복잡한 대체물을 제거하는 데 사용됩니다. 빈도 특징을 사용하여 Zipf 값이 3 미만인 경우 대체 항목이 고려 대상에서 제거됩니다. 그런 다음 LSBert는 각 특징에 대한 점수에 따라 다양한 순위를 계산합니다. 각 특징에 대한 모든 순위를 얻은 후 LSBert는 모든 순위를 평균화하여 각 후보의 점수를 매깁니다. 마지막으로, 우리는 가장 높은 순위를 가진 후보를 최고의 대안으로 선택합니다.
** Zipf(Zipf's law): 지프의 법칙



Previous work for this step is based on the following features: word frequency, contextual simplicity and Ngram language modeling, etc. In contrast to previous work, in addition to the word frequency and word similarity commonly used in other LS methods, LSBert considers three additional high-quality features: two features about Bert and one feature about PPDB (A Paraphrase Database for Simplification).
이 단계의 이전 작업은 단어 빈도, 문맥 단순성 및 N-gram 언어 모델링 등의 기능을 기반으로 합니다. 이전 작업과 달리 다른 LS 방법에서 일반적으로 사용되는 단어 빈도 및 단어 유사성 외에도 LSBert는 세 가지 추가 요소를 고려합니다. 고품질 특징: Bert에 대한 두 가지 기능과 PPDB(A Paraphrase Database for Simplification)에 대한 기능.



Bert prediction order. On this step of substitute generation, we obtain the probability distribution of the vocabulary corresponding to the mask word. Because LSBert already incorporates the context information on the step of substitution generation, the word order of Bert prediction is a crucial feature which includes the information of both the context and the complex word itself. The higher the probability, the more relevant the candidate for the original sentence.
버트 예측 순서. 이 대체 생성 단계에서 마스크 단어에 해당하는 어휘의 확률 분포를 얻습니다. LSBert는 이미 대체 생성 단계에서 문맥 정보를 포함하고 있기 때문에 Bert 예측의 어순은 문맥 정보와 복합어 자체의 정보를 모두 포함하는 중요한 특징입니다. 확률이 높을수록 원래 문장의 후보와 관련성이 높아집니다.



#### Language model feature.
A substitution candidate should fit into the sequence of words preceding and following the original word. We cannot directly compute the probability of a sentence or sequence of words using Bert like traditional n-gram language models. Let W = w-m, …, w-1, w, w1, …, wm be the context of the original word w. We adopt a new strategy to compute the likelihood of W. We first replace the original word w with the substitution candidate. We then mask one word of W from front to back and feed into Bert to compute the cross-entropy loss of the mask word. Finally, we rank all substitute candidates based on the average loss of W. The lower the loss, the substitute candidate is a good substitution for the original word. We use as context a symmetric window of size five around the complex word.
#### 언어 모델 특징.
대체 후보는 원래 단어 앞뒤의 단어 시퀀스에 맞아야 합니다. 전통적인 n-gram 언어 모델처럼 Bert를 사용하여 문장이나 단어 시퀀스의 확률을 직접 계산할 수 없습니다. W = w-m, …, w-1, w, w+1, …, w+m을 원래 단어 w의 문맥이라고 합시다. W의 가능성을 계산하기 위해 새로운 전략을 채택합니다. 먼저 원래 단어 w를 대체 후보로 바꿉니다. 그런 다음 W의 한 단어를 앞에서 뒤로 마스크하고 Bert에 공급하여 마스크 단어의 교차 엔트로피 손실을 계산합니다. 마지막으로 W의 평균 손실을 기반으로 모든 대체 후보의 순위를 지정합니다. 손실이 낮을수록 대체 후보는 원래 단어에 대한 좋은 대체입니다. 복잡한 단어 주위에 크기가 5인 대칭 window를 컨텍스트로 사용합니다.



#### Semantic similarity.
The similarity between the complex word and the substitution candidate is widely used as a feature for SR. In general, word embedding models are used to obtain the vector representation and the cosine similarity metric is chosen to compute the similarity. Here, we choose the pretrained fastText model as word embedding modeling. The higher the similarity value, the higher the ranking.
#### 의미적 유사성.
복합어와 대체 후보의 유사성은 SR의 특징으로 널리 사용됩니다. 일반적으로 단어 임베딩 모델은 벡터 표현을 얻는 데 사용되며 유사성을 계산하기 위해 코사인 유사성 메트릭이 선택됩니다. 여기에서 사전 훈련된 fastText 모델을 단어 임베딩 모델링으로 선택합니다. 유사도 값이 높을수록 순위가 높아집니다.



#### Frequency feature.
Frequency-based candidate ranking strategies are one of the most popular choices by lexical simplification and quite effective. In general, the more frequency a word is used, the most familiar it is to readers. We adopt the Zipf scale created from the SUBTLEX lists [35], because some experiments [22] revealed that word frequencies from this corpus correlate with human judgments on simplicity than many other more widely used corpora, such as Wikipedia. SUBTLEX is composed of over six million sentences extracted from subtitles of assorted movies. The Zipf frequency of a word is the base-10 logarithm of the number of times it appears per billion words.
#### 빈도 특징.
빈도 기반 후보 순위 전략은 어휘 단순화에 의해 가장 인기 있는 선택 중 하나이며 매우 효과적입니다. 일반적으로 단어를 더 많이 사용할수록 독자에게 가장 친숙합니다. 우리는 SUBTLEX 목록[35]에서 만든 Zipf 척도를 채택했습니다. 일부 실험[22]에서 이 말뭉치의 단어 빈도가 Wikipedia와 같이 널리 사용되는 다른 말뭉치보다 단순성에 대한 인간의 판단과 상관 관계가 있음이 밝혀졌기 때문입니다. SUBTLEX는 다양한 영화의 자막에서 추출한 600만 개 이상의 문장으로 구성되어 있습니다. 단어의 Zipf 빈도는 10억 단어당 나타나는 횟수의 밑수 10 로그입니다.



### Algorithm 1. Lexical simplification framework
1: S ← Input Sentence
2: t ← Complexity threshold
3: ignore list ← Named Entity Identification(S)
4: LSBert(S,t,ignore list)

1: S ← 입력 문장
2: t ← 복잡성 임계값
3: 무시할 목록 ← 명명된 개체 식별(고유명사)(S)
4: LSBert(S, t, ignore list) \# LSBert(입력 문장, 복잡성 임계값, 무시할 목록)



#### PPDB feature.
Some LS methods generated substitute candidates from PPDB or its subset SimplePPDB [8], [36]. PPDB is a collection of more than 100 million English paraphrase pairs [37]. These pairs were extracted using a bilingual pivoting technique, which assumes that two English phrases that translate to the same foreign phrase have the same meaning. Since LSBert has a better substitution generation than PPDB and SimplePPDB, they cannot help improve the performance of substitution generation. Considering PPDB owns useful information about paraphrase, we try to use PPDB as a feature to rank the candidate substitutions. We adopt a simple strategy for PPDB to rank the candidates. For each candidate ci in C of w, the ranking of ci is 1 if the pair (w, ci) exists in PPDB. Otherwise, the ranking number of ci is n/3.
#### PPDB 특징.
일부 LS 방법은 PPDB 또는 해당 하위 집합 SimplePPDB에서 대체 후보를 생성했습니다[8], [36]. PPDB는 1억 개 이상의 영어 의역 쌍의 모음입니다[37]. 이 쌍은 이중 언어 피벗 기술을 사용하여 추출되었으며, 동일한 외국어 구문으로 번역되는 두 개의 영어 구문이 동일한 의미를 갖는다고 가정합니다. LSBert는 PPDB 및 SimplePPDB보다 대체 생성 기능이 우수하므로 대체 생성 성능을 향상시킬 수 없습니다. PPDB가 의역에 대한 유용한 정보를 보유하고 있다는 점을 고려하여 PPDB를 후보 대체 순위를 지정하는 기능으로 사용하려고 합니다. 우리는 PPDB가 후보자 순위를 매기는 간단한 전략을 채택합니다. w의 C에 있는 각 후보 ci에 대해 PPDB에 (w, ci) 쌍이 존재하는 경우 ci의 순위는 1입니다. 그렇지 않으면 ci의 순위 번호는 n/3입니다.



## 3-4. LSBert Algorithm
## 3-4. LSBert 알고리즘



Following CWI, substitute generation, filtering and substitute ranking steps, the overall simplification algorithm LSBert is shown in Algorithm 1 and Algorithm 2. Given the sentence S and complexity threshold t, we first identify named entity using entity identification system. We add entities into ignore list which means these words do not need to be simplified.
CWI(Complex Word Identification), 대체 생성, 필터링 및 대체 순위 단계에 따라 전체 단순화 알고리즘 LSBert가 알고리즘 1 및 알고리즘 2에 표시됩니다. 문장 S와 복잡성 임계값 t가 주어지면 먼저 개체 식별 시스템를 사용하여 명명된 개체를 식별합니다. 무시 목록은 이러한 단어를 단순화할 필요가 없음을 의미합니다.



In LSBert, we identify all complex words in sentence s using CWI step excluding ignore list (line 1). If the number of complex words in the sentence s is larger than 0 (line 2), LSBert will try to simplify the top complex word w (line 3). LSBert calls substitute generation (line 4) and substitute ranking (line 5) in turn. LSBert chooses the top substitute (line 6). One important thing to notice is whether LSBert performs the simplification only if the top candidate top has a higher frequency (Frequency feature) or lower loss (Language model feature) than the original word (line 7). When LSBert performs the simplification, it will replace w into top (line 8) and add the word top into ignore list (line 9). After completing the simplification of one word, we will iteratively call LSBert (line 10 and line 12). If the number of complex words in S equals to 0, we will stop calling LSBert (line 15).
LSBert에서는 무시 목록(라인 1)을 제외하고 CWI 단계를 사용하여 문장의 모든 복잡한 단어를 식별합니다. 문장 s의 복잡한 단어 수가 0보다 크면(라인 2), LSBert는 상위 복잡한 단어 w(라인 3)를 단순화하려고 시도합니다. LSBert는 대체 생성(라인 4)과 대체 순위(라인 5)를 차례로 호출합니다. LSBert는 최상위 대체를 선택합니다(라인 6). 주목해야 할 한 가지 중요한 점은 상위 후보 top이 원래 단어(라인 7)보다 더 높은 빈도(Frequency feature) 또는 더 낮은 손실(Language model feature)을 갖는 경우에만 LSBert가 단순화를 수행하는지 여부입니다. LSBert가 단순화를 수행할 때 w를 top으로 바꾸고(라인 8) top이라는 단어를 무시 목록에 추가합니다(라인 9). 한 단어의 단순화를 완료한 후 LSBert를 반복적으로 호출합니다(라인 10과 라인 12). S의 복잡한 단어 수가 0이면 LSBert 호출을 중지합니다(라인 15).



#### Algorithm 2 LSBert (S,t,ignore list)
complex words ← CWI(S,t)-ignore list
if number(complex words)>0 then
    w ← head(complex words)
    subs ← Substitution Generation(S,w)
    subs ← Substitute Ranking(subs)
    top ← head(subs)
    if fre(top)>fre(w) or loss(top)<loss(w) then
        Replace(S,w,top)
        ignore list.add(w)
        LSBert(S,t,ignore list)
    else
        LSBert(S,t,ignore list)
    end if
else
    return S
end if



